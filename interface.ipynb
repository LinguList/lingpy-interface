{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Interface to Automatic Cognate Detection in LingPy.\n",
    "\n",
    "This interface will walk you through analyzing a set of linguistic data and suggest possible sets of cognates. You'll be able to download the file with analyzed cognate sets to examine yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "In order to use this interface, you'll work through cells in this Jupyter notebook, customizing the code to produce the results you want. If you're not familar with Python, or Jupyter notebooks, please start with the guide at this link: http://nbviewer.jupyter.org/github/jupyter/notebook/blob/master/docs/source/examples/Notebook/Notebook%20Basics.ipynb\n",
    "\n",
    "It'll get you familiar with how to use the rest of this notebook.\n",
    "\n",
    "### A quick refresher:\n",
    "Click on any cell to edit it. Type the needed information according to the instructions. Click \"Shift\" + \"Enter\" to evaluate a cell. All of the Python code in cells in the notebook are running together as a single program, so any time you evaluate a cell, you're adding to or changing your program. Cells can be moved or re-run in any order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations and Motivation\n",
    "This workflow is built from the workflow example in LingPy's documentation, found here: http://lingpy.org/tutorial/workflow.html. For the first several sections of this guide, the documentation and this document are largely parallel: reading them together could be helpful.\n",
    "This guide also uses code from a similar, runnable workflow that was included in LingPy 2.5, and has since been deprecated.\n",
    "\n",
    "Despite the overlap, this file functions as a standalone tool from either of these sources. It will go into more depth than the current example documentation, and provide an interactive experience to start using LingPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "First, we'll import the code we need to run this interface in LingPy. These import statements cover everything used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lingpy import *\n",
    "from lingpy.tests.util import test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and output files\n",
    "Next, we'll set up the first input and output files. We'll first pick the input file on which we want to try out automatic cognate detection. The first code cell below provides a default file to use the first time through this guide, but there are other example files listed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data files: \n",
    "- KSL.qlc\n",
    "- tutorial.qlc\n",
    "- Haspelmath-2009-1460.tsv\n",
    "- Ringe-2002-421.tsv\n",
    "- leach1969-67-161.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files can be found at the link https://github.com/lingpy/lingpy/tree/master/lingpy/tests/test_data, where you can peruse them in more detail. If you want to change the file used, you can do so here. This cell defines the input file for the guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = test_data(\"KSL.qlc\") # you can change the file name inside the quotation marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the output of LingPy's automatic cognate detection, you're going to end up with a file that contains suggested cognate sets. You can choose here what you'd like to call that output file. For now, the default assumes you used the default input file.\n",
    "\n",
    "If you'd like to change it, please provide a file name with no spaces and no extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file = \"output/KSL_output\" # by starting the string with 'output/' the files will be placed inside a folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above cell produces an error, you've asked for a test data file that does not exist. Check your spelling, and try again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Cognate Detection in LingPy: Overview\n",
    "Automatic cognate detection is the process of computationally performing the comparative method. LingPy implements several different ways to complete this analysis. \n",
    "\n",
    "The interface we're about to work through assumes familiarity with the computational method, a little Python, and enough command line knowledge to be reading this file right now.\n",
    "\n",
    "In this workflow, the first thing that will be created is a LexStat object. This is a Python object that extends the functionality of a basic word list object (called WordList) that we'll talk about how to build later. \n",
    "\n",
    "For now, all we need is an input file, and to know that LexStat is the basic class to run automatic cognate detection in LingPy. LexStat holds the data we're currently looking at, while everything we will run adds information to that object. This can be written to an output file that looks a lot like the input, with more columns added. \n",
    "\n",
    "We'll start with just an input file to create a LexStat object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lex = LexStat(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll calculate a coverage statistic first, to start understanding our dataset. We see a brief summary of the dataset from our input file, showing us each language in the dataset, and how many concepts are included in each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex.coverage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of concepts is the same for all of the languages in the dataset, we can make the initial assumption that the dataset was well curated (and probably has the same 200 concepts in each language). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring and Clustering Cognate Sets\n",
    "Next, we will look at the scorers for the dataset, from the LexStat algorithm. Scorers are computed for each pair of languages in the dataset. This allows cognate sets to be determined. The scorer defines a threshhold of similarity and cognacy for each pair of languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex.get_scorer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that scoring functions are computed, the cognate clusters across all languages in the set can be determined. This is done by running a clustering algorithm. Again, we continue with the LexStat method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex.cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to output the data to a new file. This output contains the scoring functions and a lot of computed parameters. We can look at the file here (it should be on your computer, in the same file as this notebook), however, the output isn't very readable right now. Still, we'll pause here to see what we've done so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex.output('tsv', filename=output_file, ignore=\"all\", prettify=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The last two parameters here, \"ignore\" and \"prettify\", are set to keep the output smaller. If you'd like to later, you can remove these parameters, and the resulting output file will contain the scoring functions computed for all language pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LexStat all together\n",
    "\n",
    "Now that we've looked at the current output, let's put the first few steps together. Here, we'll make a function, which performs the steps of basic cognate analysis using the steps of the LexStat we've seen so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basicLexStat(infile, outfile):\n",
    "    lex = LexStat(infile)\n",
    "    lex.coverage()\n",
    "    lex.get_scorer()\n",
    "    lex.cluster(ref=\"cognates\")\n",
    "    lex.output('tsv', filename=outfile, ignore=\"all\", prettify=False)\n",
    "    return lex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run it one more time, all at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = basicLexStat(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Now that we've detected cognates in this input data, we can evaluate the algorithm. We import the necessary methods from the evaluation package, and run the bcubes() method to calculate scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lingpy.evaluate.acd import bcubes, diff\n",
    "bcubes(lex, \"cogid\", \"cognates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B-cubed scores are a method to evaluate and compare cluster decisions (lingpy.evaluate). They are measures of similarity between two clustering processes. If there is a gold standard cognate annotation for a dataset, these scores can be used to evaluate adherence to the standard. Using a different interpretation, the scores can be used to evaluate the differences between cognate detection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignments\n",
    "The next step is to make the cognate sets more readable for manual inspection. LingPy's Alignments class will help with this. Now that each token in the input dataset has been annotated with cognate scores, indicating suggested cognate sets, we can align the sets visually in a new output file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we have to start by making an Alignments object from our existing LexStat object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alm = Alignments(lex, ref=\"cognates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a guide for the difference betweeen these classes: think of the LexStat object as thhe original dataset, with columns added containing new analysis; think of the Alignments object as the dataset with reordered rows to better highlight the results of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to run the alignment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alm.align()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the results, we use the output() method. The output options are html or tsv, and the file name can be chosen as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first argument: 'html' or 'tsv'\n",
    "# second argument: any string\n",
    "alm.output('html', filename=\"KSL\") # change filename or output type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tsv and html output files should be pretty similar. The html file is easier to read, using color and design to show alignments. The tsv file is editable, allowing for adjustments to the automatically detected cognate sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First full automatic cognate detection workflow\n",
    "Finally, we'll add the alignments steps into our workflow, so we can see everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basicACD(infile, outfile, output_type, intermediate=False):\n",
    "    lex = LexStat(infile)\n",
    "    lex.coverage()\n",
    "    lex.get_scorer()\n",
    "    lex.cluster(ref=\"cognates\")\n",
    "    if intermediate:\n",
    "        lex.output('tsv', filename=outfile+\"_intermediate\",\\\n",
    "                   ignore=\"all\", prettify=False)\n",
    "\n",
    "    alm = Alignments(lex, ref=\"cognates\")\n",
    "    alm.align()\n",
    "    alm.output(output_type, filename=outfile) # change filename or output type\n",
    "    \n",
    "    return lex, alm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: There's one new element in the workflow that we haven't talked about: the 'intermediate' parameter. Since Alignment has its own output, we don't always need output from LexStat, as well. If you'd like to see that file, you can use 'True' as the last parameter in your call to basicACD() below. If you don't want to create that output file, you don't need to include that argument at all--it will default to false, because 'intermediate=False' is included in the function defintion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex, alm = basicACD(input_file, output_file, 'html', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Parameters\n",
    "Now, we can start setting parameters to customize the cognate detection workflow. We'll go through the steps again, adding more detail along the way. We'll make the edits by declaring a method called customACD() and editing and running it repeatedly. Since we'll use the same name, the iPython notebook will re-declare the function each time.\n",
    "\n",
    "We'll start by copying in the code from basicACD()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customACD(infile, outfile, output_type, intermediate=False):\n",
    "    lex = LexStat(infile)\n",
    "    lex.coverage()\n",
    "    lex.get_scorer()\n",
    "    lex.cluster(ref=\"cognates\")\n",
    "    if intermediate:\n",
    "        lex.output('tsv', filename=outfile+\"_intermediate\",\\\n",
    "                   ignore=\"all\", prettify=False)\n",
    "\n",
    "    alm = Alignments(lex, ref=\"cognates\")\n",
    "    alm.align()\n",
    "    alm.output(output_type, filename=outfile) # change filename or output type\n",
    "    \n",
    "    return lex, alm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sound Class Models\n",
    "One of the main parameters for the LexStat is the sound class model used to encode the raw phonetic data in the input file. \n",
    "\n",
    "Sound classes are based on the idea that a sound is more likely to change to another sound that is similar to it. These similar sounds are grouped into classes, and the model is used to make judgements for correspondences between languages. \n",
    "\n",
    "LingPy allows a few different sound classes to be used in cognate detection. The available sound class models are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dolgo\n",
    "\n",
    "Dolgopolsky was one of the first linguists to study sound change by using sound classes. He proposed ten sound classes: labial obstruents, dental obstruents, sibilants, velar obstruents and dental and alveolar affricates; labial nasals; the remaining nasals; liquids; voiced labial fricative and intial rounded vowels; palatal approximants; laryngeals and inital velar nasals. To use the 'Dolgo' method, we encode data into strings that mark what sound classes each sound belongs to. \n",
    "\n",
    "##### SCA\n",
    "\n",
    "This is the default sound class model, developed as part of the LexStat method in LingPy. The sound classes used in SCA are largely based on the Dolgopolsky method (dolgo), but is extended further with an eleventh class to cover diacritics and vowels.\n",
    "\n",
    "Source for Dolgo and SCA:\n",
    "https://marija.gforge.uni.lu/esslli2010stus_submission_10.pdf\n",
    "\n",
    "##### ASJP\n",
    "\n",
    "The ASJP method and model is a standardized orthography (ASJPcode) for language comparison. The sounds of any language in this model are transcribed into characters in ASJPcode, consisting of 7 vowel symbols and 34 consonant symbols. Languages with more than 7 vowels transcibed with this model have multiple vowels represented by a single symbol. Similarly, for consonants, languages have rarer consonants merged with the more common similar sounds.\n",
    "\n",
    "Source for ASJP:\n",
    "https://www.researchgate.net/profile/Soren_Wichmann/publication/40853552_Explorations_in_automated_language_classification/links/09e41508a4943d4255000000.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to go directly to the source documentation, it's available here: http://lingpy.org/docu/data/model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've read about the models, you can add this parameter to our cognate detection workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_model_choice = \"dolgo\" # choose 'sca', 'dolgo', or 'asjp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can redefine our workflow to include the new parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customACD(infile, outfile, output_type, model_choice, \n",
    "              intermediate=False):\n",
    "    lex = LexStat(infile, model=model_choice)\n",
    "    lex.coverage()\n",
    "    lex.get_scorer()\n",
    "    lex.cluster(ref=\"cognates\")\n",
    "    if intermediate:\n",
    "        lex.output('tsv', filename=outfile+\"_intermediate\",\\\n",
    "                   ignore=\"all\", prettify=False)\n",
    "\n",
    "    alm = Alignments(lex, ref=\"cognates\")\n",
    "    alm.align()\n",
    "    alm.output(output_type, filename=outfile) # change filename or output type\n",
    "    \n",
    "    return lex, alm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this is the first custom parameter we're focusing on. Remember, the output file is at this point still probably the same, so running the new method will write over the old output. If you'd like to chance this, redefine the variable 'output_file' on a new line of code, or replace the parameter 'output_file' with a string of your choice. The same idea goes for any new customization you'd like to make!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll run the new version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex, alm = customACD(input_file, output_file, 'html', user_model_choice, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful LexStat parameter is 'check'. This turns on error checking for the input file given. Errors might be unexpected characters, or bad formatting. By setting 'check' to 'True', an error log will be created, letting you know if there's anything you should look at in your input file. We'll add it to our workflow here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customACD(infile, outfile, output_type, model_choice, \n",
    "              check_choice=False,\n",
    "              intermediate=False):\n",
    "    lex = LexStat(infile, model=model_choice, check=check_choice)\n",
    "    lex.coverage()\n",
    "    lex.get_scorer()\n",
    "    lex.cluster(ref=\"cognates\")\n",
    "    if intermediate:\n",
    "        lex.output('tsv', filename=outfile+\"_intermediate\",\\\n",
    "                   ignore=\"all\", prettify=False)\n",
    "\n",
    "    alm = Alignments(lex, ref=\"cognates\")\n",
    "    alm.align()\n",
    "    alm.output(output_type, filename=outfile) # change filename or output type\n",
    "    \n",
    "    return lex, alm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex, alm = customACD(input_file, output_file,\n",
    "                     'html', user_model_choice, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Several parameters for LexStat allow for simple changes like renaming the headings of columns in output files. \n",
    "- Parameter= 'segments'  default= \"tokens\", the column containing segmented transcriptions\n",
    "- Parameter= 'transcription'  default= \"ipa\", the column containing unsegmented token transcriptions\n",
    "- Parameter= 'classes'  default= \"classes\", the column containing the sound class representation of token transcriptions\n",
    "- Parameter= 'numbers'  default= \"numbers\", the column containing triples with numeric identifiers for tokens: language id, sound class string, and prosodic string\n",
    "\n",
    "Now, try it yourself! Here's another copy of our workflow function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customACD(infile, outfile, output_type, model_choice, \n",
    "              check_choice=False,\n",
    "              intermediate=False):\n",
    "    # add a parameter in the following line to rename a heading in \n",
    "    # your output!\n",
    "    lex = LexStat(infile, model=model_choice, check=check_choice)\n",
    "    lex.coverage()\n",
    "    lex.get_scorer()\n",
    "    lex.cluster(ref=\"cognates\")\n",
    "    if intermediate:\n",
    "        lex.output('tsv', filename=outfile+\"_intermediate\",\\\n",
    "                   ignore=\"all\", prettify=False)\n",
    "\n",
    "    alm = Alignments(lex, ref=\"cognates\")\n",
    "    alm.align(mode='dialign')\n",
    "    alm.output(output_type, filename=outfile) # change filename or output type\n",
    "    \n",
    "    return lex, alm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adding a new parameter to the function declaration. Make sure to add it to the LexStat() call on the first line as well.\n",
    "\n",
    "Remember that you can start a function parameter on a new line, like 'intermediate' above, if that's easier to read. Also remember than any parameters with a default option, like intermediate, must be at the end of the list!\n",
    "\n",
    "Finally, run your new version of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex, alm = customACD(input_file, output_file,\n",
    "                     'html', user_model_choice, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Alignments parameters\n",
    "We'll look at one more parameter before we move on. This one is used on the Alignments object, 'alm'. 'Mode' is a parameter defining alignment analysis type. The options are \"global\" and \"dialign\": whether alignment analysis weights global similarities more heavily, or maximizes local similarities. Varying this parameter could produce different results, providing new perspective, or a different baseline for exploring a dataset. The sources are listed below. Let's try it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, copy and paste your most recent version of customACD() into\n",
    "# this cell, after the comment.\n",
    "#\n",
    "# Change the following line:  \n",
    "#\n",
    "# alm.align()\n",
    "#\n",
    "# adding the parameter mode. It should look something like this:\n",
    "#\n",
    "# alm.align(mode='dialign')\n",
    "#\n",
    "# Remember, you can add the new 'mode' parameter as an argument to the \n",
    "# customACD() function declaration!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global alignment method (\"global\"):\n",
    "https://s3.amazonaws.com/academia.edu.documents/25023781/6.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1516673409&Signature=GxRLVNQiZPQONdBTlT5RGCuu9vY%3D&response-content-disposition=inline%3B%20filename%3DA_general_method_applicable_to_the_searc.pdf\n",
    "\n",
    "Local alignment method (\"dialign\"):\n",
    "http://www.pnas.org/content/93/22/12098.full.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going forward\n",
    "### More customization\n",
    "Now, we've looked at the steps of automatic cognate detection in LingPy, created a workflow, and customized the workflow with parameters. \n",
    "\n",
    "Where do you go from here?\n",
    "\n",
    "There are many more parameters and customizations to be made in LingPy. This interface has talked about a few, but more can be found starting at these links:\n",
    "- http://lingpy.org/docu/compare/lexstat.html \n",
    "- http://lingpy.org/reference/lingpy.align.html#lingpy.align.sca.Alignments\n",
    "The documentation on these classes are a good place to start in developing a more detailed workflow for research using automatic cognate detection.\n",
    "\n",
    "After looking through these classes, you can continue to explore the documentation, here: http://lingpy.org/docu/index.html to find any other useful tools LingPy offers.\n",
    "\n",
    "### Using larger datasets\n",
    "After you feel familiar using LingPy, it's time to try it out on larger datasets. The first place to start could be the files in the 'input/' directory of this repository. The following files in this directory are prepared as input to LingPy:\n",
    "- combined_fijian_maori.csv\n",
    "- slavic.tsv\n",
    "\n",
    "Try changing the definition of \"input_file\" in a new cell, and re-running customACD(). The resulting output should be more robust and complex to explore.\n",
    "\n",
    "### Using your own data\n",
    "The following tutorial in LingPy's documentation is a good start for understanding input formats: http://lingpy.org/tutorial/lingpy.basic.wordlist.html. But we'll go over some basics here:\n",
    "\n",
    "A simple input file ready to be converted into a WordList (performed by creating a LexStat object) has the following:\n",
    "- a column of tokens\n",
    "- a column of transcriptions, giving a mapping of each transcription in a language to a token concept\n",
    "- a column of doculects, marking what language each token is transcribed into\n",
    "- a gold standard annotation column\n",
    "\n",
    "The gold standard annotation column can be difficult. When using LingPy to compare automatic methods on existing hand-annotated data, one must just include the expert annotations for each word in the input file. When using LingPy to explore a new dataset, there may be no cognate sets proposed yet for the data. \n",
    "\n",
    "In this case, I suggest that as a first exploratory step, each transcription is labeled with the same id for all transcriptions of the same token. This would effectively propose that the considered languages are 100% cognate. LingPy will suggest its own cognate sets, and these can be examined to learn more about the data (as far as LingPy's methods will allow).\n",
    "\n",
    "Additionally, the Python script swadesh_scraper.py included in this repository outlines the creation of a LingPy input file, from data collection (via web scraping) to formatting. This file could be useful to explore, especially after trying out customACD() on the slavic.tsv input example. slavic.tsv is the product of the swadesh_scraper.py script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exploring output\n",
    "The html output of our customACD() workflow gives a good visual representation of the cognate sets LingPy proposes. If you'd like a little more room to explore, you can upload output in tsv form to EDICTOR at http://edictor.digling.org, an online file editor built for LingPy. Here, alignments are shown in color, can be toggled by subset, but also are able to be edited in-place. EDICTOR could be useful in the course of research using these tools.\n",
    "\n",
    "### Statistical metrics\n",
    "Remember the scoring functions introduced earlier in the notebook? These can be run on any LingPy output, and used to compare methods. Be careful, these scores are limited in what they can tell you about the analysis. Manual inspection is another good place to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Notes\n",
    "You can keep using this notebook to work with LingPy and explore! Start typing in a cell, and try out more customizations. Or start a new one, and use it to document the research process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
